{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b006ed-422a-4dec-8263-61ee7247702e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/bharathwaj.k.s@accenture.com/Mini_Project_Bharathwaj/DDL & Ingestion/Ingestion_Util\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9382bbeb-f29d-4aeb-8497-292f0561a3e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG hive_metastore;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2513ca6b-a78d-459d-a094-5d49a37f6a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %run \"./DDL & Ingestion/Ingestion_Util\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# Define your S3 credentials and config here\n",
    "# REPLACE THESE WITH YOUR ACTUAL KEYS/BUCKET NAMES\n",
    "access_key = \"AKIA4WQGEM7FTTTRW3QW\"\n",
    "secret_key = \"cz2pKF+vulpQag3t6EcbJlXDQAwv3bj1BSjxQ9Zz\"\n",
    "bucket_name = \"acen-employee-csv\"  # e.g., \"my-databricks-bucket\"\n",
    "\n",
    "# Folder paths inside the bucket (Do not include 's3a://' prefix here, the util handles it)\n",
    "source_folder_name = \"csv_file\"\n",
    "archive_folder_name = \"mini_project/Archive\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# Call the function\n",
    "# This will pick up files like 'emp05152025.csv' from S3, insert to 'rawz.Employee', and move to Archive.\n",
    "\n",
    "ingest_sequentially_s3(\n",
    "    table_name=\"Employee\",       # The destination SQL table in 'rawz' database\n",
    "    source_bucket=bucket_name,\n",
    "    source_folder=source_folder_name,\n",
    "    archive_folder=archive_folder_name,\n",
    "    file_format=\"csv\",\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Verify Data\n",
    "# %sql\n",
    "# select * from rawz.Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c825ec90-6368-4d79-ab06-f89e1f611a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rawz.employee;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b78248c-cfa7-4c18-834b-0be42d614364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from rawz.employee;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957f3848-1f9c-4780-b10c-e91e86c7b96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rawz.employee;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c43a6e-0b27-4e29-b598-04c4fab733bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- COMMAND ----------\n",
    "# -- 1. Clean up RAWZ Zone (Ingestion & Audit)\n",
    "# TRUNCATE TABLE rawz.audit_log;\n",
    "# TRUNCATE TABLE rawz.employee;\n",
    "# TRUNCATE TABLE rawz.department;\n",
    "# TRUNCATE TABLE rawz.project;\n",
    "# TRUNCATE TABLE rawz.payroll;\n",
    "# TRUNCATE TABLE rawz.training;\n",
    "\n",
    "# -- 2. Clean up WORK Zone (Transformed Data)\n",
    "# TRUNCATE TABLE work.employee;\n",
    "# TRUNCATE TABLE work.department;\n",
    "# TRUNCATE TABLE work.Project;\n",
    "# TRUNCATE TABLE work.Training;\n",
    "# TRUNCATE TABLE work.Payroll;\n",
    "\n",
    "# -- 3. Clean up CONFZ Zone (Conformed Data)\n",
    "# TRUNCATE TABLE confz.employee;\n",
    "# TRUNCATE TABLE confz.Department;\n",
    "# TRUNCATE TABLE confz.Project;\n",
    "# TRUNCATE TABLE confz.Training;\n",
    "# TRUNCATE TABLE confz.Payroll;\n",
    "\n",
    "# -- 4. Clean up PUBZ Zone (Publication/Results)\n",
    "# TRUNCATE TABLE pubz.result;\n",
    "\n",
    "# -- COMMAND ----------\n",
    "# -- 5. Verification: Check that counts are 0\n",
    "# SELECT 'rawz.audit_log' as TableName, count(*) as Count FROM rawz.audit_log\n",
    "# UNION ALL SELECT 'rawz.employee', count(*) FROM rawz.employee\n",
    "# UNION ALL SELECT 'rawz.department', count(*) FROM rawz.department\n",
    "# UNION ALL SELECT 'rawz.project', count(*) FROM rawz.project\n",
    "# UNION ALL SELECT 'rawz.payroll', count(*) FROM rawz.payroll\n",
    "# UNION ALL SELECT 'rawz.training', count(*) FROM rawz.training\n",
    "# UNION ALL SELECT 'work.employee', count(*) FROM work.employee\n",
    "# UNION ALL SELECT 'confz.employee', count(*) FROM confz.employee\n",
    "# UNION ALL SELECT 'pubz.result', count(*) FROM pubz.result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2feaa998-1eea-4e06-b9dd-8878ed22aae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #1. Define your S3 config (Same as before)\n",
    "# access_key = \"AKIA4WQGEM7FTTTRW3QW\"\n",
    "# secret_key = \"cz2pKF+vulpQag3t6EcbJlXDQAwv3bj1BSjxQ9Zz\"\n",
    "# bucket_name = \"acen-employee-csv\"  # Ensure this matches your actual bucket name\n",
    "\n",
    "# # 2. Configure Spark S3 Access\n",
    "# spark.conf.set(\"fs.s3a.access.key\", access_key)\n",
    "# spark.conf.set(\"fs.s3a.secret.key\", secret_key)\n",
    "# spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "# # 3. Define Paths\n",
    "# # Current location of files (The Archive)\n",
    "# archive_path = f\"s3a://{bucket_name}/mini_project/Archive/\"\n",
    "\n",
    "# # New location where you want them back (For testing)\n",
    "# # You mentioned creating a folder 'csv_file'\n",
    "# destination_path = f\"s3a://{bucket_name}/csv_file/\"\n",
    "\n",
    "# print(f\"Moving files FROM: {archive_path}\")\n",
    "# print(f\"Moving files TO:   {destination_path}\")\n",
    "# # 4. Move Files\n",
    "# try:\n",
    "#     # List all files in the archive\n",
    "#     files = dbutils.fs.ls(archive_path)\n",
    "    \n",
    "#     if not files:\n",
    "#         print(\"No files found in Archive to move.\")\n",
    "#     else:\n",
    "#         count = 0\n",
    "#         for file in files:\n",
    "#             # We only move actual files, not directories/success markers\n",
    "#             if file.size > 0:\n",
    "#                 source_file = file.path\n",
    "#                 # Construct destination path (dest folder + filename)\n",
    "#                 dest_file = destination_path + file.name\n",
    "                \n",
    "#                 dbutils.fs.mv(source_file, dest_file)\n",
    "#                 print(f\"Moved: {file.name}\")\n",
    "#                 count += 1\n",
    "        \n",
    "#         print(f\"--------------------------------------------------\")\n",
    "#         print(f\"RESET COMPLETE. Moved {count} files back to source.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error accessing paths: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6108397383570444,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Employee_Rawz",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
