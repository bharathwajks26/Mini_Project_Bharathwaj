{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e644843-343c-4874-b861-77b3ebb9ef15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "access_key = \"AKIA4WQGEM7FTTTRW3QW\"\n",
    "secret_key = \"cz2pKF+vulpQag3t6EcbJlXDQAwv3bj1BSjxQ9Zz\"\n",
    " \n",
    "# If your bucket is in a specific region (e.g., ap-south-1), set it here:\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.eu-north-1.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c01632-0492-4929-9811-abf9d4c26e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import current_timestamp, lit, col, to_date, coalesce, when\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "def ingest_sequentially_s3(table_name, source_bucket, source_folder, archive_folder, file_format, access_key, secret_key):\n",
    "    \"\"\"\n",
    "    Ingests files sequentially from S3.\n",
    "    Features:\n",
    "    1. STRICT SCHEMA ALIGNMENT: Prevents column swapping (Phone <-> Gender).\n",
    "    2. SMART DATE PARSING: Prevents NULL dates by handling MM/dd/yyyy vs yyyy-MM-dd.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Configure S3 Access\n",
    "    spark.conf.set(\"fs.s3a.access.key\", access_key)\n",
    "    spark.conf.set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    \n",
    "    # Allow legacy parser for loose date formats\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    \n",
    "    source_path = f\"s3a://{source_bucket}/{source_folder}/\"\n",
    "    archive_path = f\"s3a://{source_bucket}/{archive_folder}/\"\n",
    "    \n",
    "    print(f\"Checking for files in: {source_path}\")\n",
    "\n",
    "    # 2. Define Prefix Map\n",
    "    table_prefix_map = {\n",
    "        \"Employee\": \"emp\",\n",
    "        \"Department\": \"dept\",\n",
    "        \"Project\": \"project\",\n",
    "        \"Training\": \"training\",\n",
    "        \"Payroll\": \"payroll\"\n",
    "    }\n",
    "    file_prefix = table_prefix_map.get(table_name, table_name.lower()[:3])\n",
    "    print(f\"Looking for files starting with: '{file_prefix}'\")\n",
    "\n",
    "    # 3. List and Filter Files\n",
    "    try:\n",
    "        files = dbutils.fs.ls(source_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing S3 path. Check keys and bucket name.\\nError: {e}\")\n",
    "        return\n",
    "\n",
    "    data_files = []\n",
    "    date_pattern = re.compile(r\"(\\d{8})\") \n",
    "\n",
    "    for fi in files:\n",
    "        # Filter by extension and prefix\n",
    "        if fi.name.lower().endswith(f\".{file_format}\") and fi.name.lower().startswith(file_prefix):\n",
    "            match = date_pattern.search(fi.name)\n",
    "            if match:\n",
    "                date_str = match.group(1) \n",
    "                try:\n",
    "                    sort_key = datetime.strptime(date_str, \"%m%d%Y\").strftime(\"%Y%m%d\")\n",
    "                    data_files.append({\"path\": fi.path, \"name\": fi.name, \"sort_key\": sort_key})\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping file with invalid date in filename: {fi.name}\")\n",
    "\n",
    "    sorted_files = sorted(data_files, key=lambda x: x['sort_key'])\n",
    "\n",
    "    if not sorted_files:\n",
    "        print(f\"No new '{file_prefix}' files found to ingest.\")\n",
    "        return\n",
    "\n",
    "    # 4. Process Files\n",
    "    for file_info in sorted_files:\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\"Processing file: {file_info['name']}\")\n",
    "        \n",
    "        try:\n",
    "            # A. Generate Metadata\n",
    "            now = datetime.now()\n",
    "            load_key = f\"LCK{now.strftime('%Y%m%d')}{now.strftime('%H%M%S')}{random.randint(1000, 9999)}\"\n",
    "\n",
    "            # B. Read Data\n",
    "            df = spark.read.format(file_format) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .load(file_info['path'])\n",
    "            \n",
    "            # C. Add Metadata Columns\n",
    "            df = df.withColumn(\"ingestTimestamp\", current_timestamp()) \\\n",
    "                   .withColumn(\"loadKey\", lit(load_key))\n",
    "\n",
    "            # D. STRICT SCHEMA ALIGNMENT & SMART DATE CASTING\n",
    "            target_table_name = f\"rawz.{table_name}\"\n",
    "            target_schema = spark.table(target_table_name).schema\n",
    "            \n",
    "            # Map lowercase CSV columns to actual names\n",
    "            df_cols_map = {c.lower(): c for c in df.columns}\n",
    "            \n",
    "            final_select_cols = []\n",
    "            \n",
    "            for field in target_schema:\n",
    "                target_col_name = field.name\n",
    "                target_col_lower = target_col_name.lower()\n",
    "                target_dtype = field.dataType\n",
    "                \n",
    "                if target_col_lower in df_cols_map:\n",
    "                    source_col_name = df_cols_map[target_col_lower]\n",
    "                    source_col = col(source_col_name)\n",
    "                    \n",
    "                    # Logic: If Target is Date, try smart parsing. Else, standard cast.\n",
    "                    if isinstance(target_dtype, DateType):\n",
    "                        # Try MM/dd/yyyy first, then standard cast\n",
    "                        col_expr = coalesce(\n",
    "                            to_date(source_col, \"MM/dd/yyyy\"),\n",
    "                            to_date(source_col, \"M/d/yyyy\"),\n",
    "                            to_date(source_col, \"yyyy-MM-dd\"),\n",
    "                            source_col.cast(DateType()) \n",
    "                        ).alias(target_col_name)\n",
    "                    else:\n",
    "                        col_expr = source_col.cast(target_dtype).alias(target_col_name)\n",
    "                        \n",
    "                    final_select_cols.append(col_expr)\n",
    "                else:\n",
    "                    # Column missing in CSV? Insert NULL.\n",
    "                    final_select_cols.append(lit(None).cast(target_dtype).alias(target_col_name))\n",
    "            \n",
    "            # Reorder and Cast\n",
    "            df_ordered = df.select(*final_select_cols)\n",
    "\n",
    "            # E. Write to Table\n",
    "            df_ordered.write.mode(\"append\").insertInto(target_table_name)\n",
    "            \n",
    "            row_count = df_ordered.count()\n",
    "            print(f\"Successfully inserted {row_count} rows into {target_table_name}\")\n",
    "\n",
    "            # F. Audit Log\n",
    "            audit_schema = StructType([\n",
    "                StructField(\"file_name\", StringType(), True),\n",
    "                StructField(\"timestamp\", TimestampType(), True),\n",
    "                StructField(\"load_key\", StringType(), True),\n",
    "                StructField(\"completion_status\", StringType(), True),\n",
    "                StructField(\"file_path\", StringType(), True),\n",
    "                StructField(\"rows_processed\", IntegerType(), True)\n",
    "            ])\n",
    "            \n",
    "            audit_df = spark.createDataFrame([\n",
    "                (file_info['name'], datetime.now(), load_key, \"Success\", file_info['path'], row_count)\n",
    "            ], schema=audit_schema)\n",
    "            \n",
    "            audit_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"rawz.audit_log\")\n",
    "            print(\"Audit log updated.\")\n",
    "\n",
    "            # G. Move to Archive\n",
    "            destination_path = archive_path + file_info['name']\n",
    "            dbutils.fs.mv(file_info['path'], destination_path)\n",
    "            print(f\"Moved file to Archive: {destination_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED to process file {file_info['name']}: {str(e)}\")\n",
    "            print(\"Stopping sequential ingestion due to error.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db64271e-e2ae-460e-ab09-d199e20ecc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # COMMAND ----------\n",
    "# # Import necessary libraries\n",
    "# from pyspark.sql.functions import current_timestamp, lit, col\n",
    "# from pyspark.sql.types import *\n",
    "# import re\n",
    "# import random\n",
    "# from datetime import datetime\n",
    "\n",
    "# def ingest_sequentially_s3(table_name, source_bucket, source_folder, archive_folder, file_format, access_key, secret_key):\n",
    "#     \"\"\"\n",
    "#     Ingests files sequentially from S3. \n",
    "#     CRITICAL FIX: Rearranges CSV columns to match Target Table schema exactly to prevent column swapping.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1. Configure S3 Access\n",
    "#     spark.conf.set(\"fs.s3a.access.key\", access_key)\n",
    "#     spark.conf.set(\"fs.s3a.secret.key\", secret_key)\n",
    "#     spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    \n",
    "#     source_path = f\"s3a://{source_bucket}/{source_folder}/\"\n",
    "#     archive_path = f\"s3a://{source_bucket}/{archive_folder}/\"\n",
    "    \n",
    "#     print(f\"Checking for files in: {source_path}\")\n",
    "\n",
    "#     # 2. Define Prefix Map (Ensures we only pick the right files)\n",
    "#     table_prefix_map = {\n",
    "#         \"Employee\": \"emp\",\n",
    "#         \"Department\": \"dept\",\n",
    "#         \"Project\": \"project\",\n",
    "#         \"Training\": \"training\",\n",
    "#         \"Payroll\": \"payroll\"\n",
    "#     }\n",
    "#     file_prefix = table_prefix_map.get(table_name, table_name.lower()[:3])\n",
    "#     print(f\"Looking for files starting with: '{file_prefix}'\")\n",
    "\n",
    "#     # 3. List and Filter Files\n",
    "#     try:\n",
    "#         files = dbutils.fs.ls(source_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing S3 path. Check keys and bucket name.\\nError: {e}\")\n",
    "#         return\n",
    "\n",
    "#     data_files = []\n",
    "#     date_pattern = re.compile(r\"(\\d{8})\") \n",
    "\n",
    "#     for fi in files:\n",
    "#         if fi.name.lower().endswith(f\".{file_format}\") and fi.name.lower().startswith(file_prefix):\n",
    "#             match = date_pattern.search(fi.name)\n",
    "#             if match:\n",
    "#                 date_str = match.group(1) \n",
    "#                 try:\n",
    "#                     sort_key = datetime.strptime(date_str, \"%m%d%Y\").strftime(\"%Y%m%d\")\n",
    "#                     data_files.append({\"path\": fi.path, \"name\": fi.name, \"sort_key\": sort_key})\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Skipping file with invalid date: {fi.name}\")\n",
    "\n",
    "#     sorted_files = sorted(data_files, key=lambda x: x['sort_key'])\n",
    "\n",
    "#     if not sorted_files:\n",
    "#         print(f\"No new '{file_prefix}' files found to ingest.\")\n",
    "#         return\n",
    "\n",
    "#     # 4. Process Files\n",
    "#     for file_info in sorted_files:\n",
    "#         print(f\"--------------------------------------------------\")\n",
    "#         print(f\"Processing file: {file_info['name']}\")\n",
    "        \n",
    "#         try:\n",
    "#             # A. Generate Metadata\n",
    "#             now = datetime.now()\n",
    "#             load_key = f\"LCK{now.strftime('%Y%m%d')}{now.strftime('%H%M%S')}{random.randint(1000, 9999)}\"\n",
    "\n",
    "#             # B. Read Data\n",
    "#             df = spark.read.format(file_format) \\\n",
    "#                 .option(\"header\", \"true\") \\\n",
    "#                 .option(\"inferSchema\", \"true\") \\\n",
    "#                 .load(file_info['path'])\n",
    "            \n",
    "#             # C. Add Metadata Columns\n",
    "#             df = df.withColumn(\"ingestTimestamp\", current_timestamp()) \\\n",
    "#                    .withColumn(\"loadKey\", lit(load_key))\n",
    "\n",
    "#             # D. CRITICAL FIX: STRICT SCHEMA ALIGNMENT\n",
    "#             # Get the Target Table Schema (Correct Order)\n",
    "#             target_table_name = f\"rawz.{table_name}\"\n",
    "#             target_schema = spark.table(target_table_name).schema\n",
    "            \n",
    "#             # Create a Case-Insensitive Map of current DF columns\n",
    "#             # e.g. {'emp_id': 'Emp_ID', 'ph_num': 'Ph_Num'}\n",
    "#             df_cols_map = {c.lower(): c for c in df.columns}\n",
    "            \n",
    "#             final_select_cols = []\n",
    "            \n",
    "#             # Iterate through Target Table columns in ORDER\n",
    "#             for field in target_schema:\n",
    "#                 target_col_name = field.name\n",
    "#                 target_col_lower = target_col_name.lower()\n",
    "                \n",
    "#                 # Check if this target column exists in our Source DF\n",
    "#                 if target_col_lower in df_cols_map:\n",
    "#                     # Found it! Select the source column, cast it to target type, and rename it correctly\n",
    "#                     source_col_name = df_cols_map[target_col_lower]\n",
    "#                     final_select_cols.append(col(source_col_name).cast(field.dataType).alias(target_col_name))\n",
    "#                 else:\n",
    "#                     # Column missing in Source (e.g. data missing in CSV). Insert NULL safely.\n",
    "#                     # This prevents crashes if a column is missing, but keeps order intact.\n",
    "#                     print(f\"Warning: Column '{target_col_name}' missing in CSV. Inserting NULL.\")\n",
    "#                     final_select_cols.append(lit(None).cast(field.dataType).alias(target_col_name))\n",
    "            \n",
    "#             # Reorder the DataFrame to match Target Table exactly\n",
    "#             df_ordered = df.select(*final_select_cols)\n",
    "\n",
    "#             # E. Write to Table (Now safe to use insertInto)\n",
    "#             df_ordered.write.mode(\"append\").insertInto(target_table_name)\n",
    "            \n",
    "#             row_count = df_ordered.count()\n",
    "#             print(f\"Successfully inserted {row_count} rows into {target_table_name}\")\n",
    "\n",
    "#             # F. Update Audit Log\n",
    "#             audit_schema = StructType([\n",
    "#                 StructField(\"file_name\", StringType(), True),\n",
    "#                 StructField(\"timestamp\", TimestampType(), True),\n",
    "#                 StructField(\"load_key\", StringType(), True),\n",
    "#                 StructField(\"completion_status\", StringType(), True),\n",
    "#                 StructField(\"file_path\", StringType(), True),\n",
    "#                 StructField(\"rows_processed\", IntegerType(), True)\n",
    "#             ])\n",
    "            \n",
    "#             audit_df = spark.createDataFrame([\n",
    "#                 (file_info['name'], datetime.now(), load_key, \"Success\", file_info['path'], row_count)\n",
    "#             ], schema=audit_schema)\n",
    "            \n",
    "#             audit_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"rawz.audit_log\")\n",
    "#             print(\"Audit log updated.\")\n",
    "\n",
    "#             # G. Move to Archive\n",
    "#             destination_path = archive_path + file_info['name']\n",
    "#             dbutils.fs.mv(file_info['path'], destination_path)\n",
    "#             print(f\"Moved file to Archive: {destination_path}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"FAILED to process file {file_info['name']}: {str(e)}\")\n",
    "#             print(\"Stopping sequential ingestion due to error.\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512171a4-22a4-4c2f-a093-3c9eb107d8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import random\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import current_timestamp, lit, col\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "# from datetime import datetime\n",
    "\n",
    "# import boto3\n",
    "# from botocore.exceptions import ClientError\n",
    "# import re\n",
    "# from typing import List, Dict\n",
    "\n",
    "# # --------------------------\n",
    "# # S3 client factory\n",
    "# # --------------------------\n",
    "# def get_s3_client(access_key: str, secret_key: str, region: str = None):\n",
    "#     \"\"\"Return a boto3 S3 client using provided credentials.\"\"\"\n",
    "#     params = {\"aws_access_key_id\": access_key, \"aws_secret_access_key\": secret_key}\n",
    "#     if region:\n",
    "#         params[\"region_name\"] = region\n",
    "#     return boto3.client(\"s3\", **params)\n",
    "\n",
    "# # --------------------------\n",
    "# # list objects under prefix\n",
    "# # --------------------------\n",
    "# def list_s3_objects(client, bucket: str, prefix: str) -> List[Dict]:\n",
    "#     \"\"\"Return list of S3 object dicts under the prefix. Returns [] if no objects.\"\"\"\n",
    "#     objs = []\n",
    "#     paginator = client.get_paginator(\"list_objects_v2\")\n",
    "#     for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "#         for o in page.get(\"Contents\", []):\n",
    "#             objs.append(o)\n",
    "#     return objs\n",
    "\n",
    "# # --------------------------\n",
    "# # filter by table prefix (emp*.csv)\n",
    "# # --------------------------\n",
    "# def filter_table_objects(objects: List[Dict], table_prefix: str) -> List[Dict]:\n",
    "#     out = []\n",
    "#     for o in objects:\n",
    "#         key = o[\"Key\"]\n",
    "#         name = key.rstrip(\"/\").split(\"/\")[-1]\n",
    "#         if name.lower().startswith(table_prefix.lower()) and name.lower().endswith(\".csv\"):\n",
    "#             out.append(o)\n",
    "#     return out\n",
    "\n",
    "# # --------------------------\n",
    "# # extract digits after prefix and sort (earliest first)\n",
    "# # --------------------------\n",
    "# def _extract_digits_from_name(name: str, table_prefix: str) -> int:\n",
    "#     m = re.search(rf\"{re.escape(table_prefix)}(\\d+)\", name, flags=re.IGNORECASE)\n",
    "#     return int(m.group(1)) if m else 10**18\n",
    "\n",
    "# def sort_objects_by_filename_date(objs: List[Dict], table_prefix: str) -> List[Dict]:\n",
    "#     return sorted(objs, key=lambda o: _extract_digits_from_name(o[\"Key\"].split(\"/\")[-1], table_prefix))\n",
    "\n",
    "# # --------------------------\n",
    "# # download S3 object to DBFS via /dbfs/ mapping\n",
    "# # --------------------------\n",
    "# def download_s3_object_to_dbfs(client, bucket: str, key: str, dbfs_path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Download S3 object to DBFS path.\n",
    "#     - dbfs_path: example \"/mini_project/archive/emp05132025.csv\"\n",
    "#     Returns: \"dbfs:/mini_project/archive/emp05132025.csv\"\n",
    "#     \"\"\"\n",
    "#     # Local path on driver mapped to DBFS\n",
    "#     if dbfs_path.startswith(\"/\"):\n",
    "#         local_path = \"/dbfs\" + dbfs_path\n",
    "#     else:\n",
    "#         local_path = \"/dbfs/\" + dbfs_path\n",
    "\n",
    "#     # Ensure parent dir exists in DBFS\n",
    "#     parent = \"/\".join(local_path.split(\"/\")[:-1])\n",
    "#     try:\n",
    "#         dbutils.fs.mkdirs(parent.replace(\"/dbfs\",\"\"))\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # If file already exists, skip download\n",
    "#     try:\n",
    "#         with open(local_path, \"rb\"):\n",
    "#             # file exists on driver -> return dbfs path\n",
    "#             return \"dbfs:\" + dbfs_path\n",
    "#     except FileNotFoundError:\n",
    "#         pass\n",
    "\n",
    "#     # Stream download from S3 to local path\n",
    "#     try:\n",
    "#         with open(local_path, \"wb\") as f:\n",
    "#             client.download_fileobj(Bucket=bucket, Key=key, Fileobj=f)\n",
    "#     except ClientError as e:\n",
    "#         raise RuntimeError(f\"Failed to download s3://{bucket}/{key}: {e}\")\n",
    "\n",
    "#     return \"dbfs:\" + dbfs_path\n",
    "\n",
    "# # --------------------------\n",
    "# # helper: safe display list of archive\n",
    "# # --------------------------\n",
    "# def list_dbfs_path(path: str):\n",
    "#     try:\n",
    "#         return dbutils.fs.ls(path)\n",
    "#     except Exception:\n",
    "#         return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e32ad58-c6f0-4314-b6e9-4b70e9ffacfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Ingestion_Util: S3 helpers (minimal, only for source discovery + archive move)\n",
    "# import re\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# def configure_s3_keys(access_key: str, secret_key: str, region: str = \"s3.amazonaws.com\"):\n",
    "#     \"\"\"\n",
    "#     Configure Spark/Hadoop to use AWS keys on a standard (non-serverless) cluster.\n",
    "#     DO NOT commit keys to repo. Supply keys via notebook variables or Databricks secrets.\n",
    "#     \"\"\"\n",
    "#     # standard cluster: direct JVM config is allowed\n",
    "#     spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "#     spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "#     spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", region)\n",
    "#     # recommended defaults\n",
    "#     spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "#     spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "\n",
    "# def list_s3_files(bucket: str, prefix: str = \"\") -> list:\n",
    "#     \"\"\"\n",
    "#     Return dbutils-style FileInfo list for objects in s3a://<bucket>/<prefix>\n",
    "#     Example: list_s3_files(\"acen-employee-csv\", \"landing/emp/\")\n",
    "#     \"\"\"\n",
    "#     path = f\"s3a://{bucket}/{prefix}\".rstrip(\"/\") + \"/\"\n",
    "#     return dbutils.fs.ls(path)\n",
    "\n",
    "\n",
    "# def filter_table_files(fileinfo_list: list, table_prefix: str) -> list:\n",
    "#     \"\"\"\n",
    "#     Keep only files that belong to the given table. \n",
    "#     Example: table_prefix='emp' keeps emp05132025.csv\n",
    "#     \"\"\"\n",
    "#     filtered = [f for f in fileinfo_list if f.name.lower().startswith(table_prefix.lower()) and f.name.lower().endswith(\".csv\")]\n",
    "#     return filtered\n",
    "\n",
    "\n",
    "# def sort_by_filename_date(fileinfo_list: list, table_prefix: str):\n",
    "#     \"\"\"\n",
    "#     Sort by the date part after the table_prefix: empMMDDYYYY or empYYYYMMDD depending on pattern.\n",
    "#     This implementation looks for any contiguous digits after prefix and sorts by int(digits).\n",
    "#     \"\"\"\n",
    "#     def extract_digits(name):\n",
    "#         m = re.search(rf\"{re.escape(table_prefix)}(\\d+)\", name, flags=re.IGNORECASE)\n",
    "#         return int(m.group(1)) if m else 0\n",
    "#     return sorted(fileinfo_list, key=lambda f: extract_digits(f.name))\n",
    "\n",
    "\n",
    "# def move_s3_to_dbfs_archive(s3_path: str, archive_dbfs_path: str):\n",
    "#     \"\"\"\n",
    "#     Copy the file from S3 to DBFS archive and then delete the S3 source.\n",
    "#     Using dbutils.fs.cp then dbutils.fs.rm for cross-filesystem move reliability.\n",
    "#     Example: move_s3_to_dbfs_archive(\"s3a://bucket/landing/emp/emp05132025.csv\", \"/mini_project/archive/emp05132025.csv\")\n",
    "#     \"\"\"\n",
    "#     # copy\n",
    "#     dbutils.fs.cp(s3_path, archive_dbfs_path)\n",
    "#     # verify copy exists then delete the source\n",
    "#     if len(dbutils.fs.ls(archive_dbfs_path)) >= 0:\n",
    "#         dbutils.fs.rm(s3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c33582e-55d6-4f18-9bf3-df98003a188f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def insert_audit_log(\n",
    "#     spark: SparkSession,\n",
    "#     db_name: str,\n",
    "#     file_name: str,\n",
    "#     load_key: str,\n",
    "#     completion_status: str,\n",
    "#     file_path: str,\n",
    "#     rows_processed: int\n",
    "# ):\n",
    "#     audit_table = f\"{db_name}.audit_log\"\n",
    "\n",
    "#     audit_df = spark.createDataFrame(\n",
    "#         [(file_name, load_key, completion_status, file_path, rows_processed)],\n",
    "#         [\"file_name\", \"load_key\", \"completion_status\", \"file_path\", \"rows_processed\"]\n",
    "#     ).select(\n",
    "#         lit(file_name).alias(\"file_name\"),\n",
    "#         current_timestamp().alias(\"timestamp\"),\n",
    "#         lit(load_key).alias(\"load_key\"),\n",
    "#         lit(completion_status).alias(\"completion_status\"),\n",
    "#         lit(file_path).alias(\"file_path\"),\n",
    "#         lit(rows_processed).cast(\"int\").alias(\"rows_processed\")\n",
    "#     )\n",
    "\n",
    "#     audit_df.write.format(\"delta\").mode(\"append\").saveAsTable(audit_table)\n",
    "#     print(f\"Audit log inserted into {audit_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5716b630-fae9-45fd-bb4d-f9c2437b4a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def ingest_sequentially(\n",
    "#     table_name: str,\n",
    "#     source_folder: str,\n",
    "#     pk_col: str = None,\n",
    "#     db_name: str = \"rawz\",\n",
    "#     file_format: str = \"csv\",\n",
    "#     read_opts: dict = None\n",
    "# ):\n",
    "#     spark = SparkSession.builder.getOrCreate()\n",
    "#     read_opts = read_opts or {\"header\": True, \"inferSchema\": True}\n",
    "\n",
    "#     spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "#     target_table = f\"{db_name}.{table_name}\"\n",
    "#     archive_folder = f\"{source_folder}/Archive\"\n",
    "\n",
    "#     # Step 1: List and Sort Files\n",
    "#     try:\n",
    "#         files = dbutils.fs.ls(source_folder)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing source folder: {e}\")\n",
    "#         return\n",
    "\n",
    "#     table_prefix_map = {\n",
    "#         \"Employee\": \"emp\",\n",
    "#         \"Department\": \"dept\",\n",
    "#         \"Project\": \"project\",\n",
    "#         \"Training\": \"training\",\n",
    "#         \"Payroll;\": \"payroll\"\n",
    "#     }\n",
    "\n",
    "#     prefix = table_prefix_map.get(table_name, table_name.lower()[:3])\n",
    "#     csv_files = [f.path for f in files if f.name.lower().startswith(prefix) and f.name.lower().endswith(f\".{file_format}\")]\n",
    "\n",
    "#     if not csv_files:\n",
    "#         print(f\"No {file_format} files found in {source_folder} matching prefix '{prefix}'\")\n",
    "#         return\n",
    "\n",
    "#     def extract_date(path):\n",
    "#         filename = path.split(\"/\")[-1]\n",
    "#         match = re.search(rf'{prefix}(\\d{{8}})\\.{file_format}', filename)\n",
    "#         if not match:\n",
    "#             raise ValueError(f\"Filename {filename} does not match expected pattern {prefix}MMDDYYYY.csv\")\n",
    "#         return datetime.strptime(match.group(1), \"%m%d%Y\")\n",
    "\n",
    "#     try:\n",
    "#         csv_files.sort(key=extract_date)\n",
    "#     except ValueError as e:\n",
    "#         print(e)\n",
    "#         return\n",
    "\n",
    "#     file_to_process = csv_files[0]\n",
    "#     print(f\"Processing file: {file_to_process}\")\n",
    "\n",
    "#     # Step 2: Read File\n",
    "#     try:\n",
    "#         df_source = spark.read.format(file_format).options(**read_opts).load(file_to_process)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # Step 3: Check if Table Exists\n",
    "#     if not spark._jsparkSession.catalog().tableExists(db_name, table_name):\n",
    "#         print(f\"Target table {target_table} does not exist. Please create it using DDL before ingestion.\")\n",
    "#         return\n",
    "\n",
    "#     # Step 4: Generate Load Key\n",
    "#     now = datetime.now()\n",
    "#     date_part = now.strftime(\"%Y%m%d\")\n",
    "#     time_part = now.strftime(\"%H%M%S\")\n",
    "#     random_part = f\"{random.randint(1000, 9999)}\"\n",
    "#     load_key = f\"LCK{date_part}{time_part}{random_part}\"\n",
    "\n",
    "#     # Step 5: Align Schema and Insert\n",
    "#     try:\n",
    "#         df_source = df_source.withColumn(\"ingestTimestamp\", current_timestamp()) \\\n",
    "#                              .withColumn(\"loadKey\", lit(load_key))\n",
    "\n",
    "#         # Align with target table schema\n",
    "#         target_schema = spark.table(target_table).schema\n",
    "#         target_col_names = [f.name for f in target_schema]\n",
    "\n",
    "#         for field in target_schema:\n",
    "#             col_name = field.name\n",
    "#             if col_name in df_source.columns:\n",
    "#                 df_source = df_source.withColumn(col_name, df_source[col_name].cast(field.dataType))\n",
    "\n",
    "#         df_source = df_source.select(*target_col_names)\n",
    "\n",
    "#         # Write to delta\n",
    "#         df_source.write.format(\"delta\") \\\n",
    "#                        .mode(\"append\") \\\n",
    "#                        .option(\"mergeSchema\", \"false\") \\\n",
    "#                        .saveAsTable(target_table)\n",
    "\n",
    "#         completion_status = \"Success\"\n",
    "#         rows_processed = df_source.count()\n",
    "#         print(f\"Inserted {rows_processed} rows into {target_table}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error writing to Delta table: {e}\")\n",
    "#         completion_status = \"Failed\"\n",
    "#         rows_processed = 0\n",
    "\n",
    "#     # Step 6: Insert Audit Log\n",
    "#     insert_audit_log(\n",
    "#         spark=spark,\n",
    "#         db_name=db_name,\n",
    "#         file_name=file_to_process.split(\"/\")[-1],\n",
    "#         load_key=load_key,\n",
    "#         completion_status=completion_status,\n",
    "#         file_path=file_to_process,\n",
    "#         rows_processed=rows_processed\n",
    "#     )\n",
    "\n",
    "#     # Step 7: Archive File\n",
    "#     try:\n",
    "#         dbutils.fs.mv(file_to_process, f\"{archive_folder}/{file_to_process.split('/')[-1]}\")\n",
    "#         print(f\"Moved {file_to_process} to {archive_folder}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error moving file to archive: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion_Util",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
